(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{479:function(e,a,t){"use strict";t.r(a);var o=t(4),s=Object(o.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"performance-guide-for-openeo-backends"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#performance-guide-for-openeo-backends"}},[e._v("#")]),e._v(" Performance guide for openEO backends")]),e._v(" "),a("h2",{attrs:{id:"openeo-api-vision-on-performance-scalability"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#openeo-api-vision-on-performance-scalability"}},[e._v("#")]),e._v(" openEO API vision on performance & scalability")]),e._v(" "),a("p",[e._v("Given that the openEO API only defines a web service, it can in no way ensure the performance or scalability of an\nimplementation. What it can do however, is avoiding API definitions that prevent an implementation from being efficient.\nWhen openEO was designed, performance was one of the key design drivers, so here we try to explain how that is achieved.")]),e._v(" "),a("h3",{attrs:{id:"bringing-the-processing-to-the-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bringing-the-processing-to-the-data"}},[e._v("#")]),e._v(" Bringing the processing to the data")]),e._v(" "),a("p",[e._v("Two evolutions created the need for an API designed for performance and scalability: EO programmes like Copernicus that\npushed data volumes into the petabyte range, and a move towards ever larger cloud infrastructure and HPC processing capacity\nto analyze these volumes of data. OpenEO supports this by defining data access and processing into a single specification.\nAs a result, the openEO process graph allows the backend to choose any data access pattern that is optimal for\nthe processing that is to be executed, and the dataset that is to be read.")]),e._v(" "),a("p",[e._v("A popular example of such a case, is infrastructures that store the data on the same machines that do the processing.\nIn such a case, an openEO backend can choose to load and process the data directly on the machine that has the data stored.\nAnother example is adjusting and aligning the data chunks for the processing to the internal layout of the file format that\nstores the EO data. IO performance optimizations like this are only possible if the processing engine has deep knowledge\nof the data organization from storage system over networks to file formats.")]),e._v(" "),a("h3",{attrs:{id:"datacube-processing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#datacube-processing"}},[e._v("#")]),e._v(" Datacube processing")]),e._v(" "),a("p",[e._v("The datacube view that openEO uses as a model to represent the data as it is transformed by various processes also has\nimportant performance and scalability implications.")]),e._v(" "),a("p",[e._v("The easiest way to understand this is to contrast it with a more traditional 'product-based' view of building workflows.\nIn a product or file based workflow, a process operates on a set of input files and generates a set of output files. Many\nEO workflows have been written like this, but the consequence is that every process spends time on reading date into memory, and\nwriting it back to disk. Persistent storage is often the slowest component in a processing system, and thus these workflows\nspend a lot of time on IO. In the openEO specification, a process transforms one datacube into another datacube.\nBackend implementations are encouraged to avoid writing data to disk in between processing steps whenever possible. By\nkeeping the datacubes into memory, this avoids those costly IO operations.")]),e._v(" "),a("p",[e._v("Here it is important to note that openEO does not enforce or define how the datacube should look like on the backend. The\ndatacube can be a set of files, or arrays in memory distributed over a cluster. These choices are left to the\nbackend implementor, this guide only tries to highlight the possibilities.")]),e._v(" "),a("p",[e._v("For scalability, the openEO processes clearly define along which set of dimensions of the datacube they operate. When\na user writes a process graph, it should never instruct the backend to apply a black box algorithm or function on the\nentire datacube. For most algorithms, this is not necessary, and loading the complete datacube of a Copernicus mission at once\nis simply not possible. Hence, users run '"),a("a",{attrs:{href:"https://openeo.org/documentation/1.0/developers/api/reference.html#section/Processes/Process-Graphs",target:"_blank",rel:"noopener noreferrer"}},[e._v("user-defined (child) processes"),a("OutboundLink")],1),e._v("' over a 1-dimensional array, or even multidimensional arrays or 'chunks'\nof the datacube. Based on this information, the backend is able to define both a data access and processing strategy that is\noptimal for the given process graph.")]),e._v(" "),a("h2",{attrs:{id:"process-graph-execution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#process-graph-execution"}},[e._v("#")]),e._v(" Process graph execution")]),e._v(" "),a("p",[e._v("Here we go a bit more into detail about how a backend evaluates a process graph. Again, this is not normative or\nmandated by the specification, but rather an explanation of one way to achieve optimal performance.")]),e._v(" "),a("p",[e._v("In general, process graphs are first analyzed as a whole before the actual processing starts. The analysis phase serves\nto reveal the optimal processing strategy and parameters.")]),e._v(" "),a("p",[e._v("These are a few examples of things that can be derived from a process graph and subsequent optimizations:")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("Masking:")]),e._v(" when a  raster dataset is masked with another raster or polygons, then often the loading of the datacube to\nwhich the mask is applied can be limited to unmasked values.")]),e._v(" "),a("li",[a("strong",[e._v("Vector filtering:")]),e._v(" various operations (aggregate_spatial, filter_spatial, mask_polygon) can restrict the datacube to a\nset of polygons, resulting in a rather sparse cube. Loading and processing of sparse cubes can be rather different from dense data cubes.")]),e._v(" "),a("li",[a("strong",[e._v("Resampling:")]),e._v(" resampling operations can allow data to be loaded from overviews rather than original resolution. Applying\nresampling and reprojection at load time can also be faster and save memory.")]),e._v(" "),a("li",[a("strong",[e._v("Multitemporal processing:")]),e._v(" many EO algorithms work over the temporal dimension rather than spatial dimensions. The\ntype of algorithm can be inferred from the process graph, allowing to adjust the processing strategy accordingly.")])]),e._v(" "),a("h2",{attrs:{id:"performance-faq"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#performance-faq"}},[e._v("#")]),e._v(" Performance FAQ")]),e._v(" "),a("h3",{attrs:{id:"i-have-a-highly-optimized-workflow-can-openeo-expose-it"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#i-have-a-highly-optimized-workflow-can-openeo-expose-it"}},[e._v("#")]),e._v(" I have a highly optimized workflow, can openEO expose it?")]),e._v(" "),a("p",[e._v("Basically openEO can expose anything as a custom 'process'. If your algorithm can not be expressed as an openEO process graph,\nthen you can just let your backend advertise your custom process. By doing this, you still benefit from a lot of the standardized\nfeatures in the openEO API, and most tools for openEO will also support working with custom processes. This is fairly similar\nto exposing your process in other standards such as OGC Processes.")]),e._v(" "),a("p",[e._v("We do expect however that it is much more likely that your workflow can still reuse a few standardized processes. For instance,\nif it can be run on a geographical bounding box, the filter_bbox process would be a standardized way to specify that. Or\nperhaps it can operate on any set of Sentinel-2 products, in which case you might fit in a load_collection to let your users\ncustomize the input data. So usually, while you might start from a fully custom process, you'll notice that openEO offers\nways to gradually standardize your workflow further in a stepwise manner.")]),e._v(" "),a("h3",{attrs:{id:"can-openeo-be-as-fast-as-a-hand-written-workflow"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#can-openeo-be-as-fast-as-a-hand-written-workflow"}},[e._v("#")]),e._v(" Can openEO be as fast as a hand-written workflow?")]),e._v(" "),a("p",[e._v("This question depends on which backend implementation you compare to which workflow, so there is no generic answer.\nWe do note that for writing non-trivial workflows in a cloud environment, you require a combination of algorithmic programming skills\nand cloud engineering that usually requires a team of skilled persons spending (in total) multiple person months to years on the same workflow.\nSo if you know that many workflow patterns in the operational openEO backends have already been highly optimized, you may\nwant to consider if the potential of reducing processing cost with a few percentages justifies the effort.")]),e._v(" "),a("p",[e._v("Also consider that next to the openEO API, there's also a community of open source backend implementations. So if you have\nthe skills to optimize processing pipelines to perfection, why don't you consider contributing to a backend that matches your\npreferred technology stack?")])])}),[],!1,null,null,null);a.default=s.exports}}]);